import utils

import torch
import numpy as np
import dask.array as da

from scipy import sparse
from torch.distributions.normal import Normal
from torch.distributions.poisson import Poisson
from torch.distributions.distribution import Distribution
from torch.distributions.negative_binomial import NegativeBinomial
from torch.distributions.multivariate_normal import MultivariateNormal

from typing import Callable


###############################################################################
# Distributions of observations

def log_likelihood(distribution: Distribution, 
                    obs: torch.tensor) -> torch.tensor:
    """Calculates the log likelihood of the observations given a specific 
    distribution.

    Args:
        distribution (Distribution): distribution that possibly generates the
            observations.
        obs (torch.tensor): observations made.

    Returns:
        torch.tensor: log-likelihood of the observations given the distribution.
    """

    probs = distribution.log_prob(obs).sum(-1)
    return probs.sum(-1) if len(probs.shape) > 0 else probs


def subsetting_latent_var(x: torch.tensor) -> torch.tensor:
    """Subsets the latent variable such that only the subset that is used in
    generating the observations is kept.

    Args:
        x (torch.tensor): latent variable.

    Returns:
        torch.tensor: subset of the latent variable that generates the 
            observations.
    """

    n_feat = (x.shape[-1] -1) // 2
    return x[-n_feat:]


####################################
#  Negative Binomial distribution  #
####################################

def log_likelihood_neg_binom(x: torch.tensor, obs: da.array,
                                link_f: Callable,
                                theta_y: torch.tensor) -> torch.tensor:
    """Calculates the log-likelihood of the observations assuming that these
    were generated by a Negative Binomial distribution with a given mean and 
    dispersion parameter.

    Args:
        x (torch.tensor): subset of the latent variable that generates the 
            observations.
        obs (da.array): observations made.
        link_f (Callable): function that converts the latent variable into the
            mean of the Negative Binomial distribution.
        theta_y (torch.tensor): dispersion parameter of the Negative Binomial
            distribution.
        
    Returns:
        torch.tensor: log-likelihood of the latent variable assuming a Negative
            Binomial distribution.

    Requires:
        obs_i > 0
        link_f must produce only positive output values
        0 <= theta_y <= 1
    """
    n = link_f(x) * theta_y / (1 - theta_y)

    def log_pdf(y):
        log_probs = NegativeBinomial(n, 1 - theta_y).log_prob(y.a)
        return utils.DaskTensor(log_probs)
    
    likelihood = da.map_blocks(log_pdf, obs, dtype=np.float64).sum()
    return likelihood.compute().a



def sample_NegBinom_obs(eta: torch.tensor, link_f: Callable,
                        theta_y: torch.tensor, n_samples: int) -> torch.tensor:
    """Samples from a Negative Binomial distribution with the given mean and
    dispersion parameter.

    Args:
        eta (torch.tensor): subset of the latent variable that generates the 
            observations.
        link_f (Callable): function that converts the latent variable into the
            mean of the Negative Binomial distribution.
        theta_y (torch.tensor): dispersion parameter of the Negative Binomial
            distribution.
        n_samples (int): number of observations to be sampled.

    Returns:
        torch.tensor: samples (or observations) from the specified Negative
            Binomial distribution.
    
    Requires:
        link_f must produce only positive output values from eta
        0 <= theta_y <= 1
        n_samples > 0
    """
    
    n = link_f(eta) * (theta_y/ (1 - theta_y))
    return NegativeBinomial(n, 1-theta_y).sample((n_samples,))


##########################
#  Poisson distribution  #
##########################

def log_likelihood_Pois(x: torch.tensor, obs: torch.tensor, 
                        link_f: Callable) -> torch.tensor:
    """Calculates the log-likelihood of the observations assuming that these
    were generated by a Poisson distribution with a given mean
    and dispersion parameter.

    Args:
        x (torch.tensor): subset of the latent variable that generates the 
            observations.
        obs (torch.tensor): observations made.
        link_f (Callable): function that converts the subset of the latent
            variable into the mean of the Poisson distribution.

    Returns:
        torch.tensor: log-likelihood of the latent variable assuming a Poisson
            distribution.
    
    Requires:
        obs_i > 0
        link_f must produce only positive output values
    """
    
    pois_dist = Poisson(link_f(x))
    return log_likelihood(pois_dist, obs)


def sample_Pois_obs(eta: torch.tensor, link_f: Callable, 
                        n_samples: int) -> torch.tensor:
    """Samples from a Poisson distribution with the given mean and dispersion
    parameter.

    Args:
        eta (torch.tensor): subset of the latent variable that generates the 
            observations.
        link_f (Callable): function that converts the subset of the latent
            variable into the mean of the Poisson distribution.
        n_samples (int): number of observations to be sampled.

    Returns:
        torch.tensor: samples (or observations) from the specified Poisson
            distribution.
    
    Requires:
        link_f must produce only positive output values from eta
        n_samples > 0
    """
    
    mean = link_f(eta)
    return Poisson(mean).sample((n_samples, ))


###########################
#  Gaussian distribution  #
###########################

def log_likelihood_Gaus(x: torch.tensor, obs: torch.tensor,
                        link_f: Callable,
                        theta_y: torch.tensor) -> torch.tensor:
    """Calculates the log-likelihood of the observations assuming that these
    were generated by a univariate Gaussian distribution with a given mean
    and variance parameter.

    Args:
        x (torch.tensor): subset of the latent variable that generates the 
            observations.
        obs (torch.tensor): observations made.
        link_f (Callable): function that converts the subset of the latent
            variable into the mean of the Gaussian distribution.
        theta_y (torch.tensor): variance of the Gaussian distribution.

    Returns:
        torch.tensor: log-likelihood of the latent variable assuming a
            univariate Gaussian distribution.
        
    Requires:
        theta_y > 0
    """

    norm_dist = Normal(link_f(x), theta_y)
    return log_likelihood(norm_dist, obs)


def sample_Gaus_obs(eta: torch.tensor, link_f: Callable, theta_y: float,
                    n_samples: int) -> torch.tensor:
    """Samples from a univarate Gaussian distribution with the given mean and
    variance.

    Args:
        eta (torch.tensor): subset of the latent variable that generates the 
            observations.
        link_f (Callable): function that converts the subset of the latent
            variable into the mean of the Gaussian distribution.
        theta_y (torch.tensor): variance of the Gaussian distribution.
        n_samples (int): number of observations to be sampled.

    Returns:
        torch.tensor: samples (or observations) from the specified univariate
            Gaussian distribution.
        
    Requires:
        theta_y > 0
        n_samples > 0
    """
    
    mean_G = link_f(eta)
    return Normal(mean_G, theta_y).sample((n_samples,))


###############################################################################
# Distribution of the GMRF

###################################
#  Generating precision matrices  #
###################################

def creating_RW1_Q(theta_PD: torch.tensor, n_feat: int) -> torch.tensor:
    """Generates the precision matrix of a first-order Random-Walk. This 
    corresponds to an intrinsic GMRF.

    Args:
        theta_PD (torch.tensor): parameter that controls the precision of the
            Random Walk.
        n_feat (int): number of features of the Random-Walk. Side of tWhe 
            precision matrix.

    Returns:
        torch.tensor: precision matrix.
    
    Requires:
        n_feat > 0
    """
    
    theta_PD = theta_PD[None]
    main_diag = torch.hstack((theta_PD, (2*theta_PD).repeat((n_feat-2,)),
                                theta_PD))
    off_diag = theta_PD.repeat(n_feat-1)

    RW1 = torch.diag(main_diag) - torch.diag(off_diag, diagonal=-1)
    return RW1 - torch.diag(off_diag, diagonal=1)


def build_gmrf_Q(n_feat: int, theta_intercept: torch.tensor, 
                    theta_PD: torch.tensor,
                    theta_PI: torch.tensor) -> torch.tensor:
    """Constructs a precision matrix (2 * n_dim + 1) x (2 * n_dim + 1) which 
    models a gmrf x where:
        intercept ~ N(0, 1/theta_intercept)
        PD        ~ N(0, (theta_PD * R)^-1 )
        PI        ~ N(0, (theta_PI * I)^-1 )
        
        eta = intercept + PD + PI
        x   = (intercept, PD^T, eta^T)^T

        Here, I is the identity matrix of dimentions n_dim x n_dim and R is a
    first-order Random Walk graph in the form of a matrix n_dim x n_dim.

    Args:
        n_feat (int): number of dimensions of the observed variable to model.
        theta_intercept (float): parameter that controls the variance of the
            intercept of the model.
        theta_PD (float): parameter that controls the variance/influence of the
            Random Walk component of the model.
        theta_PI (float): parameter that controls the variance/influence of the 
            fixed effects component of the model.

    Returns:
        torch.tensor: precision matrix.
    """
    
    Q_11 = (theta_intercept + n_feat*theta_PI)[None]
    theta_PI_1_T = theta_PI[None].repeat((n_feat,))
    theta_PI_1 = theta_PI_1_T[:, None]

    Q_PD = creating_RW1_Q(theta_PD, n_feat)
    Q_PI = torch.diag(theta_PI_1_T)

    fst_row = torch.hstack(( Q_11      ,  theta_PI_1_T, -theta_PI_1_T))
    snd_row = torch.hstack(( theta_PI_1,   Q_PD + Q_PI, -Q_PI        ))
    trd_row = torch.hstack((-theta_PI_1,      -Q_PI   ,  Q_PI        ))

    return torch.vstack((fst_row, snd_row, trd_row))


###########################################
#  Sampling Intercept and Random Effects  #
###########################################

def sample_intercept(theta_intercept: torch.tensor, 
                        n_samples: int) -> torch.tensor:
    """Samples the intercept of a model.

    intercept ~ N(0, 1/theta_intercept)


    Args:
        theta_intercept (torch.tensor): inverse variance of the distribution
            that generates the intercept.
        n_samples (int): number of samples of the intercept.

    Returns:
        torch.tensor: sampled intercept.
        
    Requires:
        theta_y > 0
        n_samples > 0
    """

    return sample_Gaus_obs(utils.gen_tensor(0), lambda x: x, 
                            1/theta_intercept, n_samples)[:, None]


def sample_random_effects(n_feat: int, theta_PI: torch.tensor,
                            n_samples: int) -> torch.tensor:
    """Samples a random effects model. This means that it samples from the
    following distribution: N(0, 1/theta_PI * I), where I corresponds to the
    identity matrix.

    Args:
        n_feat (int): number of dimensions of the random effects to model.
        theta_PI (torch.tensor): inverse variance of the random effects model.
        n_samples (int): number of samples to generate of the random effects.

    Returns:
        torch.tensor: samples of the random effects model.
    
    Requires:
        n_feat > 0
        theta_PI > 0
        n_samples > 0
    """
    
    mean = utils.gen_tensor(0, n_feat)
    Q = torch.diag(theta_PI.repeat(n_feat,))
    
    return MultivariateNormal(mean, precision_matrix=Q).rsample((n_samples,)) 


####################
#  Sampling IGMRF  #
####################

def constraining_x(eigVal: torch.tensor, eigVec: torch.tensor, x: torch.tensor,
                    A: torch.tensor, e: torch.tensor) -> torch.tensor:
    """Changes the values of x such that it obeys the constraint:
        
        A @ x = e

    Args:
        eigVal (torch.tensor): positive eigenvalues of the precision matrix Q.
        eigVec (torch.tensor): eigenvectors associated with positive eigenvalues
            of the precision matrix Q.
        x (torch.tensor): vector whose values we want to obey the specified 
            constraint.
        A (torch.tensor): linear mapping of x that realizes the constraint.
        e (torch.tensor): result wanted for the linear mapping of x.

    Returns:
        torch.tensor: x such that it follows the constraint.
    """

    # Ax - e
    constraint = A @ x - e

    # Q^{-1}
    inv_Q = eigVec @ torch.diag(1/eigVal) @ eigVec.T

    # Q^{-1} A^T
    invQ_At = inv_Q @ A.T

    # ( A Q^{-1} A^T )^{-1}
    inv_A_invQ_At = torch.linalg.inv(A @ invQ_At)

    # x - Q^{-1} A^T ( A Q^{-1} A^T )^{-1} ( Ax - e )
    new_x = x - invQ_At @ inv_A_invQ_At @ constraint

    return new_x


def sample_IGMRF(Q: torch.tensor, n_samples: int) -> torch.tensor:
    """Samples from the intrinsic GMRF. 
    
        Given that an intrinsic GMRF is characterized by a rank deficient
    matrix (which means the precision matrix has no inverse and a 0 
    determinant), a different method has to be used to sample from it. In this
    case, we use the eigenvectors with non-zero eigenvalues associated.

    Args:
        Q (torch.tensor): graph relationships of the IGMRF in the form of a 
            matrix.
        n_samples (int): number of samples to be generated of the IGMRF.

    Returns:
        torch.tensor: Samples of the IGMRF (n_samples x Q.shape[0])
    """
    
    eigVal, eigVec = torch.linalg.eigh(Q)
    eigVal, eigVec = eigVal[1:], eigVec[:, 1:]

    cov = torch.diag(1/eigVal)
    mean = utils.gen_tensor(0, eigVal.shape[0])
    
    y = MultivariateNormal(mean, cov).sample((n_samples,)).T
    x = (eigVec @ y)

    A = utils.gen_tensor(1, Q.shape[0])[None, :]
    e = utils.gen_tensor([[0]])

    return constraining_x(eigVal, eigVec, x, A, e).T


###########################
#  sampling global model  #
###########################

def sampling_global_gmrf(n_feat: int, theta_intercept: torch.tensor, 
                            theta_PD: torch.tensor, theta_PI: torch.tensor,
                            n_samples: int) -> tuple:
    """Samples from the GMRF obtained in the following model,
    
        intercept ~ N(0, 1/theta_intercept)
        PD        ~ N(0, (theta_PD * R)^-1 )
        PI        ~ N(0, (theta_PI * I)^-1 )

        eta = intercept + PD + PI
        x = (intercept, PD^T, eta^T)^T

        where PD is a first-order Random Walk, PI is a random effects model and
    x is the global GMRF which we are sampling.

    Args:
        n_feat (int): Number of features of the PD and PI GMRFs.
        theta_intercept (float): inverse variance of the intercept.
        theta_PD (float): constant that multiplies with the structure of the
            graph of the first-order Random-Walk, forming the precision matrix
            of the Random-Walk.
        theta_PI (float): inverse variance of each of the features of the fixed-
            -effects model.
        n_samples (int): Number of samples to obtain from the global GMRF.

    Returns:
        (tuple): samples of the global GMRF.
            intercept (np.ndarray): samples from the intercept
            PD (np.ndarray): samples from the first-order Random-Walk IGMRF.
            eta (np.ndarray): samples of the combined GMRF.
    """
    
    intercept = sample_intercept(theta_intercept, n_samples)

    Q_PD = creating_RW1_Q(theta_PD, n_feat)
    PD = sample_IGMRF(Q_PD, theta_PD, n_samples)

    PI = sample_random_effects(n_feat, theta_PI, n_samples)

    eta = intercept + PD + PI
    return (intercept, PD, eta)