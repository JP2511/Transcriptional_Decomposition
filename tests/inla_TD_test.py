import sys
import torch
import unittest
import functools

import numpy as np

from typing import Callable
from itertools import product

###############################################################################

sys.path[0] += "/../src/TranscriptDecomPy/"
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

import inla_TD as tdp
import distributions_TD as dist_TD


###############################################################################

class TestConditionalDensities(unittest.TestCase):

    ####################################################
    # Defining some class variables for data synthesis #
    ####################################################

    n_feat = 100
    n_obs = 1000

    theta_y = tdp.gen_tensor(0.4).requires_grad_(True)
    theta_intercept = tdp.gen_tensor(150).requires_grad_(True)
    theta_PD = tdp.gen_tensor(150).requires_grad_(True)
    theta_PI = tdp.gen_tensor(150).requires_grad_(True)

    Q = dist_TD.build_gmrf_Q(n_feat, theta_intercept, theta_PD, theta_PI)


    intercept, PD, eta = dist_TD.sampling_global_gmrf(n_feat, theta_intercept,
                                                        theta_PD, theta_PD, 1)
    obs = dist_TD.sample_NegBinom_obs(eta[0], torch.exp, theta_y, n_obs)

    gmrf = torch.hstack((intercept[0], PD[0], eta[0]))
    

    #######################################
    #  end of class variable definitions  #
    #######################################


    # ---------------------------------------------- #
    #  tests for the function newton_raphson_method  #
    # ---------------------------------------------- #


    def test_p_x_given_y_theta(self):
        """Tests the Newton-Raphson method by testing if it correctly 
        approximates the latent vector that both was generated and is used to
        generate the observations."""

        init_v = tdp.gen_tensor(0, self.n_feat * 2 + 1)
        
        link_f = lambda x: torch.exp(dist_TD.subsetting_latent_var(x))
        data_likelihood = functools.partial(dist_TD.log_likelihood_neg_binom,
                                            obs=self.obs, link_f=link_f,
                                            theta_y=self.theta_y)
        
        mode_x, _ = tdp.p_x_given_y_theta(data_likelihood,
                                                Q=self.Q,
                                                init_v=init_v)
        
        return torch.testing.assert_close(mode_x, self.gmrf, rtol=1, atol=0.1)



    # -------------------------------------------------- #
    #  tests for the function approx_marg_post_of_theta  #
    # -------------------------------------------------- #

    def neg_p_theta_given_y(self, curr_theta: torch.tensor) -> float:
        """Creates the function that calculates the -log p(theta | y) for the
        generated data.

        Args:
            curr_theta (int): parameters used in the calculation of the 
                probability.

        Returns:
            float: - log p(theta | y)
        """
        theta_y, theta_intercept, theta_PD, theta_PI = curr_theta
        new_Q = dist_TD.build_gmrf_Q(self.n_feat, theta_intercept, theta_PD,
                                        theta_PI)

        init_v = tdp.gen_tensor(0, self.n_feat * 2 + 1)
        link_f = lambda x: torch.exp(dist_TD.subsetting_latent_var(x))
        data_likelihood = functools.partial(dist_TD.log_likelihood_neg_binom,
                                            obs=self.obs, link_f=link_f,
                                            theta_y=theta_y)
            
        mode_x, ga_L = tdp.p_x_given_y_theta(data_likelihood, new_Q, init_v)
        
        gmrf_prior = tdp.create_gmrf_density_func(new_Q)
        p_theta_y = tdp.approx_marg_post_of_theta(data_likelihood,
                                                    theta=curr_theta,
                                                    gmrf_likelihood=gmrf_prior,
                                                    theta_dist=lambda _: 1,
                                                    gaus_approx_mean=mode_x,
                                                    gaus_approx_L=ga_L)
        
        # return -p_theta_y.cpu()[0]
        return -p_theta_y
    

    def functional_bound(self, theta: torch.tensor) -> torch.tensor:
        """Function that converts the thetas generated by the minimization
        function into thetas that fall into the valid parameter space domain in
        which they exists. Here, I use the sigmoid function to restrict theta_y
        into the [0, 1] interval, and use the exponential function to convert
        the rest of thetas into the [0, +inf) space.

        Args:
            theta (torch.tensor): parameters of the model.

        Returns:
            torch.tensor: bounded parameters.
        """

        theta_y, *gmrf_theta = theta
        theta_y = torch.sigmoid(theta_y)
        if torch.round(theta_y, decimals=1) == 0:
            theta_y += 0.1

        gmrf_theta = torch.exp(torch.hstack(gmrf_theta)) + 0.1
        return torch.hstack((theta_y, gmrf_theta))


    # def test_approx_marg_post_of_theta(self):
    #     """Tests the approximation of the marginal posterior of theta by 
    #     checking that the function can successfully give higher probability to 
    #     the parameter that is actually used in generating the data. 
    #         Since this is based on sampling, it might give the wrong result 
    #     sometimes. However, increasing the number of samples should reduce the
    #     odds of this happening."""

    #     correct_result = (self.theta_y, self.theta_intercept, self.theta_PD,
    #                         self.theta_PI)
        
    #     results = []
    #     range_common=np.arange(50, 160, 20)
    #     y_range = np.arange(0.2, 0.7, 0.1)
        
    #     poss = []
    #     poss_gen = product(y_range, range_common, range_common, range_common)
    #     for curr_theta in poss_gen:
    #         theta_y, *theta = curr_theta
    #         curr_theta = (np.round(theta_y, 1), *theta)
    #         poss.append(curr_theta)

    #         res = self.neg_p_theta_given_y(tdp.gen_tensor(curr_theta))
    #         results.append(-res)
            
    #     final_choice = np.argmax(results.cpu().numpy())
    #     return self.assertTupleEqual(poss[final_choice], correct_result)
    

    def test_calc_mode_of_marg_post_theta(self):
        """Tests the calc_mode_of_marg_post_theta function, to see if it can 
        find the right mode of the density function."""
        
        init_guess = torch.hstack((tdp.gen_tensor(0.5), tdp.gen_tensor(1, 3)))
        
        result = tdp.calc_mode_of_marg_post_theta(
                    p_theta_given_y=self.neg_p_theta_given_y,
                    bounding_func=self.functional_bound,
                    init_guess=init_guess
                    )

        print("Theta Approximated:")
        print(result)


###############################################################################

unittest.main()